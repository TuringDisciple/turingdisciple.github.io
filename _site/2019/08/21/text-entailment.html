<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Solving Textual Entailment with the DecAtt Model</title>
  <meta name="description" content="Introduction In this post I will write about the problem of textual entailment and how one of my favourite models has been used to tackle the problem.">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="shortcut icon" type="image/png" href="/favicon.png">
  <link rel="canonical" href="http://localhost:4000/2019/08/21/text-entailment.html">
  <link rel="alternate" type="application/rss+xml" title="nashpotato" href="/feed.xml">
  
  
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

</head>
<body><header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" href="/">nashpotato</a>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about.html">About</a>
            
          
            
            
          
            
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Solving Textual Entailment with the DecAtt Model</h1>
    <p class="post-meta">
      <time datetime="2019-08-21T00:00:00+01:00" itemprop="datePublished">
        
        Aug 21, 2019
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <h1 id="introduction">Introduction</h1>
<p>In this post I will write about the problem of textual entailment and how one of my favourite models has been used to tackle the problem.</p>

<h1 id="what-is-textual-entailment">What is Textual Entailment?</h1>
<p>The problem of textual entailment is simple. Given a premise $p$ and a hypothesis $q$, 
can you infer the logical relationship between the pairs. $p$ and $q$ are assumed 
to be grammatically correct sentences and the inferred logical relationship is in one 
of 3 different classes.</p>

<ol>
  <li>Entailment: $p\rightarrow q$</li>
  <li>Contradiction $\neg p\rightarrow q$</li>
  <li>Neutral: cannot infer any logical relationship</li>
</ol>

<p>An example shows the problem a bit more clearly. Given the sentence $\textit{“President Trump visited Detroit, Michigan”}$ an example of sentences which would meet the three categories is given below</p>

<ol>
  <li>Entailing sentence: $\textit{The President was in a US state}$</li>
  <li>Contradicting sentence: $\textit{The President has never left the white house}$</li>
  <li>Neutral sentence: $\textit{Preying mantises have been shown to be cannibalistic}$</li>
</ol>

<h1 id="research-attempts">Research Attempts</h1>
<p>It turns out that this problem within the field of NLP can be tackled with deep learning and machine learning methods. Some of the more interesting attempts that have come to solve this problem over the past several years will be discussed alongside the DecAtt model.</p>

<h2 id="datasets">Datasets</h2>
<p>There are currently 3 primary datasets that have been used to benchmark performance on the textual entailment task.</p>

<ol>
  <li><a href="https://pdfs.semanticscholar.org/1fd8/0b5adeec4d5e921c7499a50c2cfc5b9686ad.pdf">SICK</a>: this was proposed as the dataset to use for the 2014 SemEval task 1 dataset. No longer used as the standard dataset, it did serve a purpose as putting forward the textual entailment task in a more formalised matter. This features 10,000 sentence pairs/</li>
  <li><a href="https://nlp.stanford.edu/projects/snli/">SNLI</a>: most likely considered the golden standard for the textual entailment task. This dataset features 500,000 sentence pairs, annotated and accuracy of model prediction is the primary metric.</li>
  <li><a href="https://www.nyu.edu/projects/bowman/multinli/">MNLI</a>: another standard dataset, can be used in conjunction with SNLI dataset. The reason why this dataset differs from SNLI is that it deals with multi-genre which adds a layer of difficulty compared to SNLI as all SNLI assumes all text assumes the same style.</li>
</ol>

<h2 id="decatt-2016">DecAtt (2016)</h2>
<p>Proposed in <a href="https://arxiv.org/pdf/1606.01933v1.pdf">2016 by Parikh et.al</a>, this model is notably worth discussing due to it’s high performance against its very low complexity. Compared to the <a href="https://www.aclweb.org/anthology/N16-1108">PWIM</a> model which uses almost a whole factor of 10 more parameters, it outperforms this model considerably.</p>

<p>The model works by using several feed-forward neural networks, which serve to attend on different aligned sub-parts of phrases. First by converting input sentences to 200D input encoding, the pairs of sentences go through three steps.</p>

<p><em>Attend</em>: the attend step is the first step and what it does is it takes the input sentences represented as $\mathbf{a}=(a_1, …, a_{l_a})$ and $\mathbf{b}=(b_1, …, b_{l_a})$ and using a variant of neural attention and decomposes the problem into a comparison of aligned sub-phrases. First an un-normalized attention matrix with entries denoted as $e_{ij}$ is computed as the following 
 <script type="math/tex">e_{ij} = F(a_i)^TF(b_i)</script>
 It should be noted that in the above equation, $F$ is a feed-forward neural network with ReLU activations. The attention weights are then normalised as follows</p>

<script type="math/tex; mode=display">\beta_i = \sum^{l_b}_{j=1} \frac{exp(e_{ij})}{\sum^{l_b}_{k=1}exp(e_{ik})}b_j\\
     \alpha_j = \sum^{l_a}_{i=1} \frac{exp(e_{ij})}{\sum^{l_a}_{k=1}exp(e_{kj})}a_i\\</script>

<p>$\beta_i$ is the subphrase in $\mathbf{b}$ softly aligned to $a_i$ and vice-versa for $\alpha_i$.</p>

<p><em>Compare</em>: In the compare step, aligned phrases are separately compared.</p>

<script type="math/tex; mode=display">\mathbf{v}_{1, i} := G([a_i, \beta_i])\ \ \forall i \in [1, ..., l_a]\\
\mathbf{v}_{2, j} := G([b_j, \alpha_j]) \ \ \forall i \in [1, ..., l_b]</script>

<p>Note that in the above $G$ is another feed-forward neural network. $[., .]$ brackets denote concatenation.</p>

<p><em>Aggregate</em>:
There are now two comparison vectors $\mathbf{v_1}, \mathbf{v_2}$ . These are aggregated over</p>

<script type="math/tex; mode=display">\mathbf{v_1} = \sum^{l_a}_{i=1}\mathbf{v_{1, i}}\ \ \ \ \ , \ \ \ \ \ 
\mathbf{v_2} = \sum^{l_b}_{j=1}\mathbf{v_{1, j}}</script>

<p>and fed through a final feed-forward neural network $H$ which provides a label array $\mathbf{y}\in \mathbb{R}^D$
<script type="math/tex">\mathbf{y} = H([\mathbf{v}_1, \mathbf{v}_2])</script></p>

<p>All other information regarding choice of performace, training parameters and computational complexity can be found in the original paper <a href="https://arxiv.org/pdf/1606.01933v1.pdf">here</a>. The paper reports <strong>86%</strong> accuracy on the SNLI dataset.</p>

<h2 id="conclusion">Conclusion</h2>
<p>What is interesting about this model is that infers logical relationships between sentence pairs by training a model to approximate attention weightings within a sentence pair matrix. This is not necessarily a unique idea, and models such as <a href="https://arxiv.org/pdf/1609.06038.pdf">ESIM</a> try to do a similar thing. But these require a much greater computational load due their use of LSTMs. Another model, briefly mentioned, called <a href="https://www.aclweb.org/anthology/N16-1108">PWIM</a> takes a novel approach to this problem by converting the problem into a computer vision problem by using a convolutional neural net.</p>

<p>In 2019, the best performing models for this task are <a href="https://paperswithcode.com/sota/natural-language-inference-on-snli">very large deep learning</a> models of the BERT variety, but it is worth considering that there is still potential research to be done into simpler approaches.</p>


  </div>

  
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</article>


      </div>
    </main><footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">nashpotato</h2>

    <div class="footer-col-wrapper">
      

      <div class="footer-col footer-col-1">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/nashpotato"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">nashpotato</span></a>

          </li>
          
          
          <li>
                <a href="https://twitter.com/_nash_potato"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">_nash_potato</span></a>

          </li>
          
           
          <li><a class="u-email" href="mailto:nashpotato1@protonmail.com">nashpotato1@protonmail.com</a></li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
